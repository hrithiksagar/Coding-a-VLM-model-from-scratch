{"cells":[{"cell_type":"markdown","metadata":{},"source":["### Contrastive Learning\n","Inputs will be from \"Image\" and \"Text\" --> converted to vector embeddings\n","\n","https://youtu.be/vAmKB7iPkWw\n","![Contrastive Learning](Notes/Contrastive_Learning.jpg)\n","\n","![Contrastive Learning](Notes/Contrastive_Learning_2.png)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\"\"\"\n","image_encoder = ResNet or Vision transformer\n","text_encoder =  CBOW or Text Transformer\n","I[n, h, w, c] = mini batch of aligned images\n","T[n, l] = mini batch of aligned TEXTS\n","W_i[d_i, d_e] = learned projections of image to embedding space\n","W_t[d_t, d_e] = learned projections of text to embedding space\n","t = learned temperature parameter\n","# ![Contrastive Learning](Notes/Contrastive%20Learning.jpg)\n","\"\"\"\n","# I = Image \n","# T = Text\n","# Extract feature representation of each modality [Text and Image]\n","I_f = image_encoder(I) # [n, d_i] # convert list of IMAGES into list of embeddings\n","T_t = text_encoder(T)  # [n, d_t] # convert list of TEXTS into list of embeddings\n","\n","# Joint multimodal embeddings [n, d_e]\n","    # Make sure both images and text embeddings have same number of dimensions and then normalize the vectors\n","I_e = l2_normalize(np.dot(I_f, W_i), axis = 1) # Normalize Image_Embeddings\n","T_e = l2_normalize(np.dot(T_f, W_t), axis = 1) # Normalize Text_Embeddings\n","\n","\n","# Scaled pairwise cosine similarities [n,n]\n","    # Compute all the possible dot products\n","logits = np.dot(I_e, T_e.T)*np.exp(t) # .T does Transpose, exp = exponent\n","\n","# Symmetric loss function\n","    # teach the model which item in each rows/column needs to be maximized\n","labels = np.arrange(n)\n","loss_i = cross_entropy_loss(logits, labels, axis=0) # row\n","loss_t = cross_entropy_loss(logits, labels, axis=1) # col\n","loss = (loss_i+loss_t)/2 # average both losses. \n"," we want labels to be maximum "]},{"cell_type":"markdown","metadata":{},"source":["### Problem?\n","How do we tell the model that we want only one item in a in each row/col to be the maximized while the rest of the items should be minimized?\n","\n","Hint: This is very similar to language modeling in which we want a single token to be the next one given the prompt\n","\n","Sol: \n","cross entropy loss"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["Refer this python file:\n","\n","Go through the code in\n","\n","modeling_siglip.py \n","\n","\n","![Probelm with CLIP](Notes/problem_with_clip.jpg)\n","\n","![Vision Transformer Architecture](Notes/Vision_Trasnformer_Architecture.JPG)"]},{"cell_type":"markdown","metadata":{},"source":["## Batch Normalization\n","![Batch Norm 1](Notes/Batch_Norm_1.JPG)\n","\n","![Batch Norm 2](Notes/Batch_Norm_2.JPG)\n"]},{"cell_type":"markdown","metadata":{},"source":["## Layer Normalization\n","![Layer Norm ](Notes/Layer_Norm.JPG)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"venv1","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.6"}},"nbformat":4,"nbformat_minor":2}
